{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Je1apZ-P9Pe",
        "GxG_EhsAQCEz",
        "HDyZxjc0QFt9",
        "bcWNwDZ8U6ge",
        "mCPz9zVtU9_X",
        "3w16BnXMVBbb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "7Je1apZ-P9Pe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPPrt3jqMYMj"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import clip\n",
        "import joblib\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from torchvision import transforms\n"
      ],
      "metadata": {
        "id": "aIU8AMn2MeD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "GxG_EhsAQCEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/photos.tsv000\", sep=\"\\t\")\n",
        "urls = df[\"photo_image_url\"].dropna().tolist()\n",
        "image_urls = [url + \"?w=512\" for url in urls]"
      ],
      "metadata": {
        "id": "U-AHhEjbQBq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Pipeline"
      ],
      "metadata": {
        "id": "HDyZxjc0QFt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "q7C02U78MfkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/clip_embeddings.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "bSM2ewb7MjOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cq_k-b4eMkgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "CHUNK_SIZE = 1000\n",
        "SAVE_DIR = \"/content/clip_embeddings\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "existing_files = set(os.listdir(SAVE_DIR))\n",
        "\n",
        "for chunk_start in range(0, len(image_urls), CHUNK_SIZE):\n",
        "    chunk_end = min(chunk_start + CHUNK_SIZE, len(image_urls))\n",
        "    chunk_name = f\"embeddings_{chunk_start}_{chunk_end}.pt\"\n",
        "\n",
        "    if chunk_name in existing_files:\n",
        "        print(f\"Skipping already saved: {chunk_name}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing chunk: {chunk_start} to {chunk_end}\")\n",
        "\n",
        "    image_embeddings = []\n",
        "    valid_indices = []\n",
        "    batch_images = []\n",
        "    batch_indices = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for rel_idx, url in enumerate(tqdm(image_urls[chunk_start:chunk_end], desc=\"Processing Chunk\")):\n",
        "        idx = chunk_start + rel_idx\n",
        "        try:\n",
        "            response = requests.get(url, timeout=5)\n",
        "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "            processed = preprocess(img)\n",
        "\n",
        "            batch_images.append(processed)\n",
        "            batch_indices.append(idx)\n",
        "\n",
        "            if len(batch_images) == BATCH_SIZE or idx == chunk_end - 1:\n",
        "                input_batch = torch.stack(batch_images).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    features = model.encode_image(input_batch)\n",
        "                    features = features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                image_embeddings.extend(features.cpu())\n",
        "                valid_indices.extend(batch_indices)\n",
        "\n",
        "                batch_images = []\n",
        "                batch_indices = []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed at {idx}: {e}\")\n",
        "\n",
        "    if image_embeddings:\n",
        "        save_path = os.path.join(SAVE_DIR, chunk_name)\n",
        "        torch.save({\n",
        "            \"embeddings\": torch.stack(image_embeddings),\n",
        "            \"indices\": valid_indices\n",
        "        }, save_path)\n",
        "        print(f\"Saved {len(image_embeddings)} embeddings to {save_path}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Chunk {chunk_start}-{chunk_end} done in {(end - start)/60:.2f} minutes.\")\n",
        "\n",
        "    # Exit after one chunk (remove this break if you want to process all at once)\n",
        "    # break\n"
      ],
      "metadata": {
        "id": "Ni1luU85Mn3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dir = \"/content/clip_embeddings/\"\n",
        "all_embeddings = []\n",
        "all_indices = []\n",
        "\n",
        "for file in sorted(os.listdir(embedding_dir)):\n",
        "    if file.endswith(\".pt\"):\n",
        "        data = torch.load(os.path.join(embedding_dir, file))\n",
        "        all_embeddings.append(data[\"embeddings\"])\n",
        "        all_indices.extend(data[\"indices\"])\n",
        "\n",
        "embedding_matrix = torch.cat(all_embeddings, dim=0).numpy()\n",
        "print(f\"Loaded {embedding_matrix.shape[0]} embeddings.\")"
      ],
      "metadata": {
        "id": "QAVSVKwCPpwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = NearestNeighbors(n_neighbors=6, metric=\"cosine\")\n",
        "knn.fit(embedding_matrix)\n",
        "print(\"KNN model fitted.\")"
      ],
      "metadata": {
        "id": "AWEGY4oSPrPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_urls = df[\"photo_image_url\"].dropna().tolist()\n",
        "image_urls = [url + \"?w=512\" for url in image_urls]\n",
        "\n",
        "image_thumbnails = []\n",
        "\n",
        "for idx in tqdm(all_indices, desc=\"ðŸ”— Storing thumbnail URLs only\"):\n",
        "    try:\n",
        "        response = requests.get(image_urls[idx], timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            image_thumbnails.append(image_urls[idx])\n",
        "        else:\n",
        "            image_thumbnails.append(None)\n",
        "    except Exception as e:\n",
        "        image_thumbnails.append(None)"
      ],
      "metadata": {
        "id": "4-P6KCzXPtO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "72eVs7aKQO5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_and_show_matches(url,\n",
        "                                model,\n",
        "                                knn,\n",
        "                                image_thumbnails,\n",
        "                                device,\n",
        "                                n_neighbors=30,\n",
        "                                top_k=6):\n",
        "    \"\"\" Given an image URL, this function computes its CLIP embedding,\n",
        "    retrieves the top-k similar images, and visualizes the matches.\n",
        "\n",
        "    Args:\n",
        "        url (str):  URL of the input image\n",
        "        model: Loaded CLIP model.\n",
        "        knn: Trained NearestNeighbors model on image embeddings\n",
        "        image_thumbnails (list): List of image thumbnail URLs (same order as embeddings)\n",
        "        device: torch.device ('cuda' or 'cpu')\n",
        "        n_neighbors (int, optional): Number of nearest neighbors to search. Defaults to 30\n",
        "        top_k (int, optional): Number of matches to display. Defaults to 6\n",
        "    \"\"\"\n",
        "\n",
        "    input_image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    input_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_embedding = model.encode_image(input_tensor)\n",
        "        input_embedding = input_embedding / input_embedding.norm(dim=-1, keepdim=True)\n",
        "    input_embedding_np = input_embedding.cpu().numpy()\n",
        "\n",
        "    _, all_indices = knn.kneighbors(input_embedding_np, n_neighbors=n_neighbors)\n",
        "\n",
        "    def resize_image(img, size=(224, 224)):\n",
        "        return img.resize(size)\n",
        "\n",
        "    plt.figure(figsize=(10, 9))\n",
        "    plt.subplot(3, 3, 2)\n",
        "    plt.imshow(resize_image(input_image))\n",
        "    plt.title(\"Your Input\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    match_count = 0\n",
        "    plot_position = 4\n",
        "\n",
        "    for idx in all_indices[0]:\n",
        "        if image_thumbnails[idx] is None:\n",
        "            continue\n",
        "        try:\n",
        "            match_img = Image.open(\n",
        "                BytesIO(requests.get(image_thumbnails[idx]).content)).convert(\"RGB\")\n",
        "            resized_match = resize_image(match_img)\n",
        "            plt.subplot(3, 3, plot_position)\n",
        "            plt.imshow(resized_match)\n",
        "            plt.title(f\"Match {match_count + 1}\")\n",
        "            plt.axis(\"off\")\n",
        "            match_count += 1\n",
        "            plot_position += 1\n",
        "            if match_count == top_k:\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    plt.tight_layout(pad=0.5)\n",
        "    plt.subplots_adjust(wspace=0.05, hspace=0.3)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-qk4227zPu63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_image_and_show_matches(\n",
        "    url=\"https://upload.wikimedia.org/wikipedia/commons/e/e7/Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg\",\n",
        "    model=model_loaded\n",
        "    knn=knn_loaded,\n",
        "    image_thumbnails=image_thumbnails_loaded,\n",
        "    device=device\n",
        "    n_neighbors=30,\n",
        "    top_k=6\n",
        ")\n"
      ],
      "metadata": {
        "id": "UTXvSoDLP3Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name='similar_image_model.sav'\n",
        "model_data = {\n",
        "    'knn_model': knn,\n",
        "    'embedding_matrix': embedding_matrix,\n",
        "    'image_thumbnails': image_thumbnails,\n",
        "    'all_indices': all_indices,\n",
        "    'image_urls': image_urls\n",
        "}\n",
        "\n",
        "with open(file_name, 'wb') as f:\n",
        "  pickle.dump(model_data, f)"
      ],
      "metadata": {
        "id": "8i9uxhjjk3Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "FjToQsN2QQr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have saved our model, embeddings, indexes and image url as sav. We will load and do the evaluation."
      ],
      "metadata": {
        "id": "q5Y1ErjgTocx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_name, 'rb') as f:\n",
        "  loaded_model = pickle.load(f)\n",
        "\n",
        "knn_loaded = loaded_model['knn_model']\n",
        "embedding_matrix_loaded = loaded_model['embedding_matrix']\n",
        "image_thumbnails_loaded = loaded_model['image_thumbnails']\n",
        "all_indices_loaded = loaded_model['all_indices']\n",
        "image_urls_loaded = loaded_model['image_urls']"
      ],
      "metadata": {
        "id": "8QJ7CtFrTn34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP Similarity Score"
      ],
      "metadata": {
        "id": "bcWNwDZ8U6ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_clip_similarity_scores(embedding_matrix, knn_model, num_queries=1000, k=6):\n",
        "    total_images = len(embedding_matrix)\n",
        "    query_indices = random.sample(range(total_images), num_queries)\n",
        "\n",
        "    clip_scores = []\n",
        "    score_lookup = {}\n",
        "\n",
        "    for query_idx in query_indices:\n",
        "        query_embed = embedding_matrix[query_idx].reshape(1, -1)\n",
        "        retrieved_indices = knn_model.kneighbors(query_embed, n_neighbors=k)[1][0]\n",
        "        retrieved_embeds = embedding_matrix[retrieved_indices]\n",
        "        sim_scores = cosine_similarity(query_embed, retrieved_embeds)[0]\n",
        "        avg_sim = np.mean(sim_scores)\n",
        "        clip_scores.append(avg_sim)\n",
        "        score_lookup[query_idx] = avg_sim\n",
        "\n",
        "    return clip_scores, query_indices, score_lookup"
      ],
      "metadata": {
        "id": "pF0RYXY0TM8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clip_similarity(clip_scores):\n",
        "    avg_score = np.mean(clip_scores)\n",
        "    min_score = np.min(clip_scores)\n",
        "    max_score = np.max(clip_scores)\n",
        "\n",
        "    print(f\"âœ… CLIP Similarity Summary (Top 6 avg over {len(clip_scores)} queries):\")\n",
        "    print(f\"Average Score : {avg_score:.4f}\")\n",
        "    print(f\"Max Score     : {max_score:.4f}\")\n",
        "    print(f\"Min Score     : {min_score:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    # Histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(clip_scores, bins=25, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(avg_score, color='red', linestyle='dashed', linewidth=1)\n",
        "    plt.title(\"CLIP Similarity Histogram\")\n",
        "    plt.xlabel(\"Avg CLIP Cosine Similarity (Top 6)\")\n",
        "    plt.ylabel(\"Number of Queries\")\n",
        "\n",
        "    # Boxplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.boxplot(clip_scores, vert=False)\n",
        "    plt.title(\"Boxplot of CLIP Similarities\")\n",
        "    plt.xlabel(\"Avg CLIP Cosine Similarity\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bH1lFzbkTT-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intra-query diversity score"
      ],
      "metadata": {
        "id": "mCPz9zVtU9_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_intra_query_diversity(embedding_matrix, knn_model, query_indices, k=6):\n",
        "    diversity_scores = []\n",
        "\n",
        "    for query_idx in query_indices:\n",
        "        query_embed = embedding_matrix[query_idx].reshape(1, -1)\n",
        "        retrieved_indices = knn_model.kneighbors(query_embed, n_neighbors=k)[1][0]\n",
        "        embeds = embedding_matrix[retrieved_indices]\n",
        "        sims = cosine_similarity(embeds)\n",
        "        n = sims.shape[0]\n",
        "        upper_tri_similarities = [sims[i, j] for i in range(n) for j in range(i+1, n)]\n",
        "        avg_sim = np.mean(upper_tri_similarities)\n",
        "        diversity = 1 - avg_sim\n",
        "        diversity_scores.append(diversity)\n",
        "\n",
        "    return diversity_scores\n"
      ],
      "metadata": {
        "id": "49VSsmxLTVp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_diversity_scores(diversity_scores):\n",
        "    avg_div = np.mean(diversity_scores)\n",
        "    print(f\"âœ… Average Diversity Score: {avg_div:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(diversity_scores, bins=25, color='lightgreen', edgecolor='black')\n",
        "    plt.title(\"Intra-query Diversity Score Distribution (1 - Avg Pairwise Similarity)\")\n",
        "    plt.xlabel(\"Diversity Score (0 = very similar, 1 = very different)\")\n",
        "    plt.ylabel(\"Number of Queries\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TGsJrTwgTW4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP Similarity v Intra-query diversity scores"
      ],
      "metadata": {
        "id": "3w16BnXMVBbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_similarity_vs_diversity(clip_scores, diversity_scores):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(clip_scores, diversity_scores, alpha=0.6, color='purple')\n",
        "    plt.title(\"CLIP Similarity vs. Intra-query Diversity\")\n",
        "    plt.xlabel(\"Avg CLIP Similarity (Top 6)\")\n",
        "    plt.ylabel(\"Diversity Score\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eEqMMrY7TYGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_scores, query_indices, score_lookup = compute_clip_similarity_scores(\n",
        "    embedding_matrix=embedding_matrix_loaded,\n",
        "    knn_model=knn_loaded,\n",
        "    num_queries=1000,\n",
        "    k=6\n",
        ")\n",
        "\n",
        "diversity_scores = compute_intra_query_diversity(\n",
        "    embedding_matrix=embedding_matrix_loaded,\n",
        "    knn_model=knn_loaded,\n",
        "    query_indices=query_indices,\n",
        "    k=6\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G4xI7cUmTKdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_diversity_scores(diversity_scores)\n"
      ],
      "metadata": {
        "id": "sGKGxN5DUjMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_clip_similarity(clip_scores)\n"
      ],
      "metadata": {
        "id": "ZyO4uUhVUka7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_similarity_vs_diversity(clip_scores, diversity_scores)\n"
      ],
      "metadata": {
        "id": "5DXYlE-DUlae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE Visualization"
      ],
      "metadata": {
        "id": "KbFBS9tEVHE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tsne_for_multiple_queries(embedding_matrix, knn_model, num_queries=50, background_size=2000, k=6):\n",
        "\n",
        "    total_images = len(embedding_matrix)\n",
        "    query_sample_indices = random.sample(range(total_images), num_queries)\n",
        "\n",
        "    all_query_and_matches = []\n",
        "    for q in query_sample_indices:\n",
        "        top_k = knn_model.kneighbors(embedding_matrix[q].reshape(1, -1), n_neighbors=k)[1][0]\n",
        "        all_query_and_matches.extend([q] + list(top_k))\n",
        "\n",
        "    background_indices = random.sample(range(total_images), background_size)\n",
        "    all_indices = list(set(background_indices + all_query_and_matches))\n",
        "\n",
        "    embeds = embedding_matrix[all_indices]\n",
        "    tsne_result = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(embeds)\n",
        "    index_to_2d = {idx: tsne_result[i] for i, idx in enumerate(all_indices)}\n",
        "\n",
        "    colors = cm.rainbow(np.linspace(0, 1, num_queries))\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    bg_coords = np.array([index_to_2d[idx] for idx in background_indices if idx in index_to_2d])\n",
        "    plt.scatter(bg_coords[:, 0], bg_coords[:, 1], c='lightgray', s=10, label='Other Images')\n",
        "\n",
        "    for i, q_idx in enumerate(query_sample_indices):\n",
        "        color = colors[i]\n",
        "        if q_idx not in index_to_2d:\n",
        "            continue\n",
        "        q_coord = index_to_2d[q_idx]\n",
        "        plt.scatter(q_coord[0], q_coord[1], marker='X', color=color, s=100, edgecolor='black', label=f\"Query {i+1}\")\n",
        "\n",
        "        top_k = knn_model.kneighbors(embedding_matrix[q_idx].reshape(1, -1), n_neighbors=k)[1][0]\n",
        "        for match_idx in top_k:\n",
        "            if match_idx in index_to_2d:\n",
        "                match_coord = index_to_2d[match_idx]\n",
        "                plt.scatter(match_coord[0], match_coord[1], color=color, s=40)\n",
        "\n",
        "    plt.title(f\"t-SNE: {num_queries} Queries and Their Top-{k} Matches\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7KxVfxpVUs2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne_for_multiple_queries(\n",
        "    embedding_matrix=embedding_matrix_loaded,\n",
        "    knn_model=knn_loaded,\n",
        "    num_queries=1,\n",
        "    background_size=2000,\n",
        "    k=6\n",
        ")\n"
      ],
      "metadata": {
        "id": "IETVFlQ8VXP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}